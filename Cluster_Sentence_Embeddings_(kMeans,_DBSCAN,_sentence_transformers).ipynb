{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cluster Sentence Embeddings (kMeans, DBSCAN, sentence-transformers).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSGnxnLV4uKgk5otNOkbnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raymondwcs/learning_bert/blob/master/Cluster_Sentence_Embeddings_(kMeans%2C_DBSCAN%2C_sentence_transformers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axSRYWSicNeN",
        "outputId": "021c1169-5049-4d22-bec6-e4118a4f9200"
      },
      "source": [
        "!pip install --quiet transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.12)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.9.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wrrWqI_ccPW"
      },
      "source": [
        "# from transformers import AutoModel, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# CHECKPOINT = 'bert-base-chinese'\n",
        "CHECKPOINT = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "\n",
        "MAX_LENGTH = 80\n",
        "\n",
        "# corpus = [\n",
        "#   \"Vodafone Wins ₹20,000 Crore Tax Arbitration Case Against Government\",\n",
        "#   \"Voda Idea shares jump nearly 15% as Vodafone wins retro tax case in Hague\",\n",
        "#   \"Gold prices today fall for 4th time in 5 days, down ₹6500 from last month high\",\n",
        "#   \"Silver futures slip 0.36% to Rs 59,415 per kg, down over 12% this week\",\n",
        "#   \"Amazon unveils drone that films inside your home. What could go wrong?\",\n",
        "#   \"IPHONE 12 MINI PERFORMANCE MAY DISAPPOINT DUE TO THE APPLE B14 CHIP\",\n",
        "#   \"Delhi Capitals vs Chennai Super Kings: Prithvi Shaw shines as DC beat CSK to post second consecutive win in IPL\",\n",
        "#   \"French Open 2020: Rafael Nadal handed tough draw in bid for record-equaling 20th Grand Slam\"\n",
        "# ]\n",
        "\n",
        "corpus = [\n",
        "  '這個服務生很不親切',         \n",
        "  '这个服务生很不亲切', \n",
        "  '黄昏時滂沱大雨',  \n",
        "  '放工時下大雨',\n",
        "  '這個週末陽光普照!',\n",
        "  '天朗氣清的星期天。',\n",
        "  '現時恒指已跌穿重要支持位26500點來看。',\n",
        "  '港股繼續尋底的機會是頗高的。'         \n",
        "]\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "# model = AutoModel.from_pretrained(CHECKPOINT)\n",
        "\n",
        "# tokens = tokenizer(text=corpus, max_length=MAX_LENGTH, add_special_tokens=True, padding='max_length', truncation=True, return_tensors='pt')\n",
        "# output = model(**tokens)\n",
        "\n",
        "corpus_embeddings = []\n",
        "# for i in range(len(output.pooler_output)):\n",
        "#   corpus_embeddings.append(output.pooler_output[i].detach().numpy())\n",
        "\n",
        "embedder = SentenceTransformer(CHECKPOINT)\n",
        "corpus_embeddings = embedder.encode(corpus)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK6r2Ua-vo8v"
      },
      "source": [
        "Embedding Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIoNU5oru6RP",
        "outputId": "8eb0781b-a1e3-4ad8-d1ca-776067f5b6ec"
      },
      "source": [
        "similarities = cosine_similarity(corpus_embeddings)\n",
        "similarities_sorted = similarities.argsort()\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "score = []\n",
        "for index,array in enumerate(similarities_sorted):\n",
        "    id_1.append(index)\n",
        "    id_2.append(array[-2])\n",
        "    score.append(similarities[index][array[-2]])\n",
        "index_df = pd.DataFrame({'id_1' : id_1,\n",
        "                          'id_2' : id_2,\n",
        "                          'score' : score})\n",
        "\n",
        "print(index_df)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id_1  id_2     score\n",
            "0     0     1  0.980593\n",
            "1     1     0  0.980593\n",
            "2     2     3  0.867589\n",
            "3     3     2  0.867589\n",
            "4     4     5  0.719055\n",
            "5     5     4  0.719055\n",
            "6     6     2  0.211332\n",
            "7     7     4  0.296934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul5-9_b-utoc"
      },
      "source": [
        "K-Means Clustering (distance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXesFBH5ez1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419c8422-d208-447d-8071-10bb84ca2d65"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 4\n",
        "# Define kmeans model\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "# Fit the embedding with kmeans clustering.\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "# Get the cluster id assigned to each news headline.\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "print(cluster_assignment)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 1 3 3 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbNfU733iFOm",
        "outputId": "8d77918b-3c81-4ac7-d6cc-aedb393837e4"
      },
      "source": [
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster  1\n",
            "['這個服務生很不親切', '这个服务生很不亲切']\n",
            "\n",
            "Cluster  2\n",
            "['黄昏時滂沱大雨', '放工時下大雨', '現時恒指已跌穿重要支持位26500點來看。']\n",
            "\n",
            "Cluster  3\n",
            "['港股繼續尋底的機會是頗高的。']\n",
            "\n",
            "Cluster  4\n",
            "['這個週末陽光普照!', '天朗氣清的星期天。']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuFMgl8awZx3"
      },
      "source": [
        "K-Means Clustering (similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkhJ5IMEsh2m",
        "outputId": "0b7a547a-eb7e-440c-e6e5-4ea17e9936b9"
      },
      "source": [
        "import nltk\n",
        "from nltk.cluster.kmeans import KMeansClusterer\n",
        "num_clusters = 4\n",
        "data = corpus_embeddings\n",
        "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
        "assigned_clusters = kclusterer.cluster(data, assign_clusters=True)\n",
        "\n",
        "print(assigned_clusters)\n",
        "print()\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 3, 3, 2, 2, 1, 2]\n",
            "\n",
            "Cluster  1\n",
            "['這個服務生很不親切', '这个服务生很不亲切']\n",
            "\n",
            "Cluster  2\n",
            "['黄昏時滂沱大雨', '放工時下大雨', '現時恒指已跌穿重要支持位26500點來看。']\n",
            "\n",
            "Cluster  3\n",
            "['港股繼續尋底的機會是頗高的。']\n",
            "\n",
            "Cluster  4\n",
            "['這個週末陽光普照!', '天朗氣清的星期天。']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN48mFEPu5uc"
      },
      "source": [
        "DBSCAN (distance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt0VZNrAWLj4",
        "outputId": "b02b79e3-8fe4-43aa-c3b2-d9d9830db733"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "cluster = DBSCAN(eps=2.25,min_samples=1).fit(corpus_embeddings)\n",
        "cluster.labels_"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 2, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}