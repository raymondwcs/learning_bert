{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis (hugging face pipeline, uer/roberta-base-finetuned-jd-full-chinese).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNywwwi5GU8FI/bXjHK1YkY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raymondwcs/learning_bert/blob/master/Sentiment_Analysis_(hugging_face_pipeline%2C_uer_roberta_base_finetuned_jd_full_chinese).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFjkIxP5DUvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3ad034-d9b3-418c-a398-ff432573d886"
      },
      "source": [
        "!pip install --quiet transformers\n",
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 58.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.3 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqj44ARNjQtP",
        "outputId": "a31dd7d8-5a3d-4460-b227-e7e086b38470"
      },
      "source": [
        "# checkpoint = 'uer/roberta-base-finetuned-jd-full-chinese'  # https://huggingface.co/uer/roberta-base-finetuned-jd-full-chinese\n",
        "checkpint = 'bert-base-chinese'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
        "sentences = [\n",
        "  '芒果大合奏~ 次次去都係食呢個，好多層次的芒果，有芒果乾，芒果布丁，芒果肉，芒果雪糕，芒果漿，不同texture, 感覺真係好rich~ 大滿足',\n",
        "  '但係我地黎既目的係食韓燒，點知種類賣相真係嚇死你！啲肉好似冇奄過咁！啲牛肉粒仲瘦過瘦肉，都算啦！點知所有野食燒完後完全冇味！',\n",
        "  '這個服務生很不親切'\n",
        "]\n",
        "\n",
        "classifier(sentences)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'star 5', 'score': 0.8894888162612915},\n",
              " {'label': 'star 2', 'score': 0.5215009450912476},\n",
              " {'label': 'star 1', 'score': 0.39362475275993347}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c97_-Pn1wz2n"
      },
      "source": [
        "# Sentence Embedding in BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytF-pxlzmo3x",
        "outputId": "dbf1ec05-fd70-41fc-bbcd-68b9d37ecbe8"
      },
      "source": [
        "sentences = [\n",
        "  '這個服務生很不親切',\n",
        "  '这个服务生很不亲切', \n",
        "  '今天是晴天今天是晴',\n",
        "  '今天是晴天',\n",
        "  '今天陽光充沛!',\n",
        "  '今天下大雨',     \n",
        "]\n",
        "\n",
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(checkpoint)\n",
        "\n",
        "tokens = tokenizer(text=sentences, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "input_ids = tokens.input_ids[0].unsqueeze(0)\n",
        "attention_mask = tokens.attention_mask[0].unsqueeze(0)\n",
        "output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "sentence_embedding_01 = output.pooler_output\n",
        "\n",
        "input_ids = tokens.input_ids[1].unsqueeze(0)\n",
        "attention_mask = tokens.attention_mask[1].unsqueeze(0)\n",
        "output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "sentence_embedding_02 = output.pooler_output\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity, pairwise_distance\n",
        "\n",
        "print(cosine_similarity(sentence_embedding_01, sentence_embedding_02))\n",
        "print(pairwise_distance(sentence_embedding_01, sentence_embedding_02))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at uer/roberta-base-finetuned-jd-full-chinese were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([0.9979], grad_fn=<DivBackward0>)\n",
            "tensor([0.8085], grad_fn=<NormBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UwuqDytX-zu",
        "outputId": "dda77e7e-3afa-42cd-e9d3-c05b57302050"
      },
      "source": [
        "output = model(input_ids=tokens.input_ids, output_hidden_states=True)\n",
        "\n",
        "# https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
        "# output[0]   # last_hidden_state\n",
        "# output[1]   # pooler_output\n",
        "# output[2]   # hidden layers (13 = input embeddings + 12 BERT layers)\n",
        "\n",
        "hidden_states = output[2]\n",
        "\n",
        "# print(hidden_states)\n",
        "print(len(hidden_states))             # 13 = input embeddings + 12 BERT layers\n",
        "print(hidden_states[-1].shape)        # last hidden layer [1, <input length>, 768]\n",
        "print(len(hidden_states[-1]))         # 1\n",
        "print(len(hidden_states[-1][0]))      # <input length>\n",
        "print(len(hidden_states[-1][0][0]))   # 768\n",
        "print(hidden_states[-1])\n",
        "\n",
        "# `hidden_states` has shape [13 x 1 x <input length> x 768]\n",
        "# `token_vecs` is a tensor with shape [<input length> x 768]\n",
        "token_vecs = hidden_states[1:13][0]\n",
        "print(len(token_vecs))\n",
        "\n",
        "# Calculate the average of all token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n",
            "torch.Size([6, 11, 768])\n",
            "6\n",
            "11\n",
            "768\n",
            "tensor([[[ 0.0627, -0.0396, -0.0781,  ...,  0.0085,  0.2588, -0.6025],\n",
            "         [ 0.2348,  0.0642, -0.0767,  ...,  0.1848,  0.4004, -0.3693],\n",
            "         [ 0.2111,  0.0940, -0.0319,  ...,  0.1745,  0.3543, -0.2807],\n",
            "         ...,\n",
            "         [ 0.1526, -0.0884, -0.0880,  ...,  0.0538,  0.2824, -0.3470],\n",
            "         [ 0.1402, -0.0732, -0.0842,  ...,  0.0567,  0.2550, -0.3416],\n",
            "         [ 0.0627, -0.0396, -0.0781,  ...,  0.0085,  0.2588, -0.6025]],\n",
            "\n",
            "        [[ 0.0344, -0.0613, -0.0784,  ..., -0.0095,  0.2935, -0.5952],\n",
            "         [ 0.1838,  0.0070, -0.0668,  ...,  0.1731,  0.4417, -0.3823],\n",
            "         [ 0.1603,  0.0282, -0.0163,  ...,  0.1786,  0.3784, -0.2861],\n",
            "         ...,\n",
            "         [ 0.1448, -0.1205, -0.0857,  ...,  0.0273,  0.2931, -0.3139],\n",
            "         [ 0.1378, -0.0929, -0.0832,  ...,  0.0504,  0.2634, -0.3347],\n",
            "         [ 0.0344, -0.0613, -0.0784,  ..., -0.0096,  0.2935, -0.5952]],\n",
            "\n",
            "        [[ 0.0187,  0.0908,  0.0059,  ..., -0.1731,  0.1371,  0.1081],\n",
            "         [ 0.0046,  0.1843, -0.0071,  ..., -0.1677,  0.2573,  0.1377],\n",
            "         [-0.0179,  0.1298, -0.0482,  ..., -0.1057,  0.1768,  0.1701],\n",
            "         ...,\n",
            "         [ 0.1168,  0.1898, -0.0659,  ..., -0.2347,  0.1729,  0.1163],\n",
            "         [ 0.0860,  0.2201, -0.1047,  ..., -0.1970,  0.1207,  0.1031],\n",
            "         [ 0.0187,  0.0908,  0.0059,  ..., -0.1731,  0.1371,  0.1080]],\n",
            "\n",
            "        [[ 0.0641,  0.0618, -0.0505,  ..., -0.2673,  0.2516,  0.3237],\n",
            "         [ 0.0366,  0.1586, -0.0199,  ..., -0.2998,  0.3027,  0.2872],\n",
            "         [ 0.0556,  0.1134, -0.0684,  ..., -0.3371,  0.2102,  0.3353],\n",
            "         ...,\n",
            "         [ 0.1794,  0.1895, -0.1047,  ..., -0.2565,  0.2682,  0.3252],\n",
            "         [ 0.1829,  0.1865, -0.0935,  ..., -0.2446,  0.2624,  0.3322],\n",
            "         [ 0.1554,  0.2191, -0.0717,  ..., -0.2229,  0.2884,  0.3207]],\n",
            "\n",
            "        [[ 0.2756,  0.0347, -0.1332,  ..., -0.2509,  0.4781,  0.6279],\n",
            "         [ 0.2042,  0.0363, -0.0682,  ..., -0.2601,  0.5038,  0.5331],\n",
            "         [ 0.1717,  0.0603, -0.1321,  ..., -0.3535,  0.4266,  0.5859],\n",
            "         ...,\n",
            "         [ 0.2756,  0.0347, -0.1332,  ..., -0.2509,  0.4781,  0.6279],\n",
            "         [ 0.3019,  0.1570, -0.1719,  ..., -0.2567,  0.4965,  0.3940],\n",
            "         [ 0.2450,  0.1322, -0.1759,  ..., -0.2292,  0.5391,  0.4409]],\n",
            "\n",
            "        [[ 0.0625,  0.0975, -0.0462,  ..., -0.2104,  0.2202,  0.3326],\n",
            "         [ 0.0985,  0.1469, -0.0470,  ..., -0.2756,  0.3313,  0.3204],\n",
            "         [ 0.0311,  0.1581, -0.1275,  ..., -0.3398,  0.2244,  0.3758],\n",
            "         ...,\n",
            "         [ 0.2406,  0.2292, -0.1070,  ..., -0.2106,  0.2593,  0.3509],\n",
            "         [ 0.2445,  0.2236, -0.0951,  ..., -0.2062,  0.2498,  0.3449],\n",
            "         [ 0.2147,  0.2514, -0.0666,  ..., -0.1918,  0.2782,  0.3397]]],\n",
            "       grad_fn=<NativeLayerNormBackward>)\n",
            "6\n",
            "Our final sentence embedding vector of shape: torch.Size([11, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Q_s9FONAHf",
        "outputId": "21a99ee8-b02c-4e49-b94e-172d7a63628c"
      },
      "source": [
        "tokens = tokenizer(text=sentences, add_special_tokens=True, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "input_ids = tokens.input_ids[0].unsqueeze(0)\n",
        "attention_mask = tokens.attention_mask[2].unsqueeze(0)\n",
        "output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "hidden_states = output[2]\n",
        "token_vecs = hidden_states[1:][0]\n",
        "sentence_embedding01= torch.mean(token_vecs,dim=0)\n",
        "\n",
        "input_ids = tokens.input_ids[1].unsqueeze(0)\n",
        "attention_mask = tokens.attention_mask[1].unsqueeze(0)\n",
        "output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "hidden_states = output[2]\n",
        "token_vecs = hidden_states[1:][0]\n",
        "sentence_embedding02= torch.mean(token_vecs,dim=0)\n",
        "\n",
        "print(sentence_embedding01.size())\n",
        "print(sentence_embedding02.size())\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity, pairwise_distance\n",
        "\n",
        "sim = cosine_similarity(sentence_embedding01, sentence_embedding02)\n",
        "torch.mean(sim, dim=0)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([11, 768])\n",
            "torch.Size([11, 768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9280, grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}